{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUG=os.environ.get(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import json\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "token = \"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\"  # Replace with your Hugging Face token\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with accelerate to manage memory\n",
    "from accelerate import infer_auto_device_map, init_empty_weights, load_checkpoint_and_dispatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model_name_or_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map='auto' if torch.cuda.is_available() else None,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    token=token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the correct environment setup\n",
    "!pip install git+https://github.com/huggingface/transformers\n",
    "!pip install datasets loralib sentencepiece\n",
    "!pip install bitsandbytes accelerate xformers einops\n",
    "!pip install --upgrade huggingface_hub\n",
    "\n",
    "# Log in to Hugging Face\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "# Verify CUDA availability\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA current device:\", torch.cuda.current_device())\n",
    "    print(\"CUDA device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Import necessary libraries\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import json\n",
    "import textwrap\n",
    "\n",
    "# Load the tokenizer\n",
    "token = \"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\"  # Replace with your Hugging Face token\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=token)\n",
    "\n",
    "# Load the model\n",
    "model_name_or_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map='auto' if torch.cuda.is_available() else None,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    token=token,\n",
    ")\n",
    "\n",
    "# Set up the text generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device=device,\n",
    "    do_sample=True,\n",
    "    top_k=30,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Define system prompt and functions\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful, and honest assistant specializing in translation and transliteration. Your primary task is to translate and transliterate text accurately while maintaining the correct grammar and context. Always ensure that your translations and transliterations are as faithful to the original meaning as possible.\"\"\"\n",
    "SYSTEM_PROMPT = B_SYS + DEFAULT_SYSTEM_PROMPT + E_SYS\n",
    "\n",
    "def get_prompt(hindi_sentence, english_words):\n",
    "    instruction = f\"\"\"\\\n",
    "Given a sentence in Hindi and a list of English words, replace the Hindi words that have similar meanings with their transliterated English equivalents. Ensure the sentence maintains correct grammar and word order.\n",
    "\n",
    "Hindi Sentence: {hindi_sentence}\n",
    "English Words: {english_words}\n",
    "\n",
    "Transformed Sentence:\"\"\"\n",
    "    prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "    return prompt_template\n",
    "\n",
    "def cut_off_text(text, prompt):\n",
    "    cutoff_phrase = prompt\n",
    "    index = text.find(cutoff_phrase)\n",
    "    if index != -1:\n",
    "        return text[:index]\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def remove_substring(string, substring):\n",
    "    return string.replace(substring, \"\")\n",
    "\n",
    "def generate(hindi_sentence, english_words):\n",
    "    prompt = get_prompt(hindi_sentence, english_words)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    final_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    final_outputs = cut_off_text(final_outputs, '</s>')\n",
    "    final_outputs = remove_substring(final_outputs, prompt)\n",
    "\n",
    "    return final_outputs\n",
    "\n",
    "def parse_text(text):\n",
    "    wrapped_text = textwrap.fill(text, width=100)\n",
    "    print(wrapped_text + '\\n\\n')\n",
    "    return wrapped_text\n",
    "\n",
    "# Sample usage\n",
    "hindi_sentence = \"मेरा नाम मौनिका है मैं राष्ट्रीय प्रौद्योगिकी संस्थान कालीकट में बी. टेक की पढ़ाई कर रही हूँ। मैं एन. आई. टी. सी. में एक मंदबुद्धि व्यक्ति से मिला। उसका नाम प्रणीत है, कोई नहीं जानता कि वह कैसे प्रतिक्रिया देगा। उसके जाने का इंतज़ार कर रहा हूँ। यह मेरी कहानी है। धन्यवाद नमस्ते।\"\n",
    "english_words = [\"national\", \"institute\", \"of\", \"technology\", \"react\", \"thanks\", \"mentally\", \"retarded\"]\n",
    "\n",
    "# Generate transformed sentence\n",
    "transformed_sentence = generate(hindi_sentence, english_words)\n",
    "parse_text(transformed_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with accelerate to manage memory\n",
    "from accelerate import infer_auto_device_map, init_empty_weights, load_checkpoint_and_dispatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "token = \"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\"  # Replace with your Hugging Face token\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model_name_or_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map='auto' if torch.cuda.is_available() else None,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    token=token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "model_name_or_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map='auto' if torch.cuda.is_available() else None,\n",
    "    load_in_8bit=True,  # This flag is from bitsandbytes\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    token=token\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "model_name_or_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Define BitsAndBytesConfig\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # Enable 8-bit loading\n",
    "    load_in_8bit_fp32_cpu_offload=True  # Offload fp32 parts to CPU\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    quantization_config=bnb_config,  # Use the BitsAndBytesConfig\n",
    "    device_map='auto' if torch.cuda.is_available() else None,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    token=token\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_name_or_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Define BitsAndBytesConfig\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # Enable 8-bit loading\n",
    ")\n",
    "\n",
    "# Define a custom device map\n",
    "device_map = {\n",
    "    \"transformer.wte\": \"cuda:0\",\n",
    "    \"transformer.h\": \"cpu\",\n",
    "    \"transformer.ln_f\": \"cuda:0\",\n",
    "    \"lm_head\": \"cuda:0\"\n",
    "}\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    quantization_config=bnb_config,  # Use the BitsAndBytesConfig\n",
    "    device_map=device_map,  # Use the custom device map\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    token=token\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
